{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis with NLTK and Gensim\n",
    "\n",
    "To install the packages in Jupyter Notebook, run the lines **!pip3 install nltk**, **!pip3 install gensim**, and **!pip3 install afinn** or pip install in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents for our corpus:\n",
    "trump = open(\"TrumpInaguration.txt\", \"r\",encoding=\"utf8\")\n",
    "obama = open(\"ObamaInaguration.txt\", \"r\",encoding=\"utf8\")\n",
    "wbush = open(\"WBushInaguration.txt\", \"r\",encoding=\"utf8\")\n",
    "clinton = open(\"ClintonInaguration.txt\", \"r\",encoding=\"utf8\")\n",
    "bush = open(\"BushInaguration.txt\", \"r\",encoding=\"utf8\")\n",
    "\n",
    "all_docs = [trump, obama, wbush, clinton, bush]\n",
    "\n",
    "# read in all of the documents and put them together in a corpus\n",
    "corpus = []\n",
    "for doc in all_docs:\n",
    "    corpus.append(doc.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set of all of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# set of punctuation \n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean(doc):\n",
    "    # Remove stop words\n",
    "    stop_free = ' '.join([term for term in doc.lower().split() if term not in stop_words])\n",
    "    #doc.lower().split(): converts everything to lower case, creates a list of all items separated with a space\n",
    "    \n",
    "    # Remove punctuation\n",
    "    punc_free = ''.join(term for term in stop_free if term not in punctuation)\n",
    "    \n",
    "    # Stem each word\n",
    "    stemmed = ' '.join(stemmer.stem(word) for word in punc_free.split())\n",
    "    \n",
    "    return stemmed\n",
    "\n",
    "# clean every document in the corpus\n",
    "clean_corpus = [clean(doc).split() for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for doc in clean_corpus:\n",
    "    all_words += doc\n",
    "\n",
    "# define \"word_frequencies\" as the frequencies of all of the words in \"all_words\"\n",
    "word_frequencies = nltk.FreqDist(all_words)\n",
    "# print the 20 most common words\n",
    "word_frequencies.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make data frame from dictionary\n",
    "term_df = pd.DataFrame({'count':list(word_frequencies.values())}, index = word_frequencies.keys())\n",
    "# sort the values so that the terms with the highest counts are at the top\n",
    "term_df.sort_values('count', ascending = False, inplace = True)\n",
    "term_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a part of speech pair for each term in the index of term_df\n",
    "tagged = nltk.pos_tag(term_df.index)\n",
    "\n",
    "print(tagged[0:20]) # print the first 20\n",
    "\n",
    "# Put the words and parts of speech in a dataframe\n",
    "terms = []\n",
    "tags = []\n",
    "for pair in tagged:\n",
    "    terms.append(pair[0])\n",
    "    tags.append(pair[1])\n",
    "\n",
    "pos_df = pd.DataFrame({'POS': tags}, index = terms)\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part of speech tags:\n",
    "\n",
    "CC - coordinating conjunction<br>\n",
    "CD - cardinal digit<br>\n",
    "DT - determiner<br>\n",
    "EX - existential there (like: \"there is\" ... think of it like \"there exists\")<br>\n",
    "FW - foreign word<br>\n",
    "IN - preposition/subordinating conjunction<br>\n",
    "JJ - adjective\t'big'<br>\n",
    "JJR - adjective, comparative\t'bigger'<br>\n",
    "JJS - adjective, superlative\t'biggest'<br>\n",
    "LS - list marker\t1)<br>\n",
    "MD - modal\tcould, will<br>\n",
    "NN - noun, singular 'desk'<br>\n",
    "NNS - noun plural\t'desks'<br>\n",
    "NNP - proper noun, singular\t'Harrison'<br>\n",
    "NNPS - proper noun, plural\t'Americans'<br>\n",
    "PDT - predeterminer\t'all the kids'<br>\n",
    "POS - possessive ending\tparent's<br>\n",
    "PRP - personal pronoun\tI, he, she<br>\n",
    "PRP\\$ - possessive pronoun\tmy, his, hers<br>\n",
    "RB - adverb\tvery, silently<br>\n",
    "RBR - adverb, comparative\tbetter<br>\n",
    "RBS - adverb, superlative\tbest<br>\n",
    "RP - particle\tgive up<br>\n",
    "TO - to\tgo 'to' the store.<br>\n",
    "UH - interjection\terrrrrrrrm<br>\n",
    "VB - verb, base form\ttake<br>\n",
    "VBD - verb, past tense\ttook<br>\n",
    "VBG - verb, gerund/present participle\ttaking<br>\n",
    "VBN - verb, past participle\ttaken<br>\n",
    "VBP - verb, sing. present, non-3d\ttake<br>\n",
    "VBZ - verb, 3rd person sing. present\ttakes<br>\n",
    "WDT - wh-determiner\twhich<br>\n",
    "WP - wh-pronoun\twho, what<br>\n",
    "WP$ - possessive wh-pronoun\twhose<br>\n",
    "WRB - wh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine the dataframes\n",
    "df = pd.concat([term_df, pos_df], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "# make an afinn object\n",
    "afinn = Afinn()\n",
    "\n",
    "# get sentiment for each document in the corpus \n",
    "# (unclean corpus because the clean one has some words the algorithm won't recognize)\n",
    "titles = ['trump', 'obama', 'wbush', 'clinton', 'bush']\n",
    "for i in range(len(corpus)):\n",
    "    print(titles[i] + ' sentiment score:', afinn.score(corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Few Ways to Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the most frequent words\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "top20 = df.iloc[:20]\n",
    "sns.barplot(x = top20['count'], y = top20.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make a word cloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# make a word cloud for each document in the corpus\n",
    "titles = ['trump', 'obama', 'wbush', 'clinton', 'bush']\n",
    "for i in range(len(clean_corpus)):\n",
    "    # generate a wordcloud object\n",
    "    wordcloud = WordCloud(width = 1000, height = 1000, background_color='white').generate(' '.join(clean_corpus[i]))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(titles[i])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wo_stem(doc):\n",
    "    # Remove stop words\n",
    "    stop_free = ' '.join([term for term in doc.lower().split() if term not in stop_words])\n",
    "    \n",
    "    # Remove punctuation\n",
    "    punc_free = ''.join(term for term in stop_free if term not in punctuation)\n",
    "    \n",
    "    return punc_free\n",
    "\n",
    "clean_wo_stem_corpus = [clean_wo_stem(doc).split() for doc in corpus]\n",
    "\n",
    "for i in range(len(clean_wo_stem_corpus)):\n",
    "    temp = pd.DataFrame({'scores':afinn.scores_with_pattern(' '.join(clean_wo_stem_corpus[i]))})\n",
    "    sns.barplot(x = temp.index, y = temp['scores'])\n",
    "    plt.title(titles[i])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "#     print(afinn.scores_with_pattern(' '.join(doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling using Latent Dirichlet Allocation\n",
    "\n",
    "![](LDA-concept.png)\n",
    "\n",
    "### Preparing a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(clean_corpus)\n",
    "# print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assigned a unique integer id to all words appearing in the corpus with the gensim.corpora.dictionary.Dictionary class. This sweeps across the texts, collecting word counts and relevant statistics. In the end, we see there are 1560 distinct words in the processed corpus, which means each document will be represented by 1560 numbers (ie., by a 1560-Dimensional vector). To see the mapping between words and their ids, uncomment the \"print(dictionary.token2id)\" line.\n",
    "\n",
    "To actually convert tokenized documents to vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "dtm = [dictionary.doc2bow(doc) for doc in clean_corpus]\n",
    "# print(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function doc2bow() simply counts the number of occurrences of each distinct word, converts the word to its integer word id and returns the result as a sparse vector. To view the vectors, uncomment the line \"print(dtm)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(dtm, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "# print out the topics\n",
    "for topic in ldamodel.show_topics(num_topics = -1, num_words = 5):\n",
    "    print(topic, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers in the topics are the probability that each word is in the topic.\n",
    "\n",
    "#### Here is a simple example of how it works:\n",
    "\n",
    "Suppose you have the following corpus:\n",
    "\n",
    "* I ate a banana and spinach smoothie for breakfast\n",
    "* I like to eat broccoli and bananas.\n",
    "* Chinchillas and kittens are cute.\n",
    "* My sister adopted a kitten yesterday.\n",
    "* Look at this cute hamster munching on a piece of broccoli.\n",
    "\n",
    "Latent Dirichlet allocation is a way of automatically discovering topics that these sentences contain. For example, given these sentences and asked for 2 topics, LDA might produce something like\n",
    "\n",
    "* Sentences 1 and 2: 100% Topic A\n",
    "* Sentences 3 and 4: 100% Topic B\n",
    "* Sentence 5: 60% Topic A, 40% Topic B\n",
    "* Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, ... (at which point, you could interpret topic A to be about food)\n",
    "* Topic B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, ... (at which point, you could interpret topic B to be about cute animals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensive Topic Modeling Website by UVA Prof:\n",
    "http://datascience.shanti.virginia.edu/polo/courses/topic/all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "\n",
    "https://medium.com/towards-data-science/word-to-vectors-natural-language-processing-b253dd0b0817\n",
    "\n",
    "https://www.digitalocean.com/community/tutorials/how-to-work-with-language-data-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "\n",
    "https://radimrehurek.com/gensim/tut1.html\n",
    "\n",
    "https://radimrehurek.com/gensim/tutorial.html#first-example\n",
    "\n",
    "https://amueller.github.io/word_cloud/auto_examples/simple.html\n",
    "\n",
    "https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
